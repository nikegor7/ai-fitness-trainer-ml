{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Fitness Trainer — Thesis Analysis\n",
    "\n",
    "**Master's Thesis:** *Development of an Intelligent System for Monitoring the Performance of Physical Exercises Using Computer Vision and Deep Learning*\n",
    "\n",
    "This notebook generates all quantitative results and figures for the thesis:\n",
    "1. Synthetic dataset statistics\n",
    "2. Model training curves\n",
    "3. Per-exercise evaluation (MAE, Pearson, ICC)\n",
    "4. Three-way comparison: Rule-based vs DL vs Hybrid\n",
    "5. Thesis figures (publication-ready)\n"
   ],
   "id": "cell-md-intro"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "ROOT = Path('..').resolve()\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "from config import EXERCISES, SCORE_DIMS, ANGLE_NAMES\n",
    "\n",
    "sns.set_theme(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "print('TensorFlow', tf.__version__)\n",
    "print('NumPy', np.__version__)\n"
   ],
   "id": "cell-imports"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1  Synthetic Dataset Statistics"
   ],
   "id": "cell-md-data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.dataset_loader import load_synthetic\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
    "fig.suptitle('Synthetic Dataset — Score Distributions', fontsize=14, fontweight='bold')\n",
    "\n",
    "colors = sns.color_palette('muted', len(EXERCISES))\n",
    "\n",
    "for i, dim in enumerate(SCORE_DIMS):\n",
    "    ax = axes.flat[i]\n",
    "    for ex, color in zip(EXERCISES, colors):\n",
    "        try:\n",
    "            _, scores = load_synthetic(ex)\n",
    "            ax.hist(scores[:, i] * 100, bins=40, alpha=0.5,\n",
    "                    label=ex.replace('_', ' '), color=color, density=True)\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "    ax.set_title(dim, fontsize=11)\n",
    "    ax.set_xlabel('Score (0–100)')\n",
    "    ax.set_ylabel('Density')\n",
    "    if i == 0:\n",
    "        ax.legend(fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(ROOT / 'evaluation/figures/dataset_score_distributions.png',\n",
    "            dpi=150, bbox_inches='tight')\n",
    "plt.show()\n"
   ],
   "id": "cell-data-distributions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics table\n",
    "rows = []\n",
    "for ex in EXERCISES:\n",
    "    try:\n",
    "        seqs, scores = load_synthetic(ex)\n",
    "        s100 = scores * 100\n",
    "        row = {'exercise': ex, 'n_samples': len(seqs)}\n",
    "        for i, dim in enumerate(SCORE_DIMS):\n",
    "            row[f'{dim}_mean'] = round(s100[:, i].mean(), 1)\n",
    "            row[f'{dim}_std']  = round(s100[:, i].std(),  1)\n",
    "        rows.append(row)\n",
    "    except FileNotFoundError:\n",
    "        rows.append({'exercise': ex, 'n_samples': 0, 'note': 'NOT FOUND'})\n",
    "\n",
    "df_stats = pd.DataFrame(rows)\n",
    "print('Synthetic dataset summary')\n",
    "display(df_stats[['exercise', 'n_samples'] +\n",
    "                 [f'{d}_mean' for d in SCORE_DIMS]])\n"
   ],
   "id": "cell-data-stats"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2  Sample Rep Trajectories"
   ],
   "id": "cell-md-trajectories"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.dataset_loader import load_synthetic\n",
    "\n",
    "fig, axes = plt.subplots(1, len(EXERCISES), figsize=(16, 4), sharey=True)\n",
    "fig.suptitle('Sample Rep Trajectories (one per exercise)', fontsize=13)\n",
    "\n",
    "for ax, ex in zip(axes, EXERCISES):\n",
    "    try:\n",
    "        seqs, scores = load_synthetic(ex)\n",
    "        # Pick a high-quality rep (overall score > 80)\n",
    "        good_idx = np.where(scores[:, 5] * 100 > 80)[0]\n",
    "        idx = good_idx[0] if len(good_idx) > 0 else 0\n",
    "        seq = seqs[idx]\n",
    "        for j, name in enumerate(ANGLE_NAMES):\n",
    "            ax.plot(seq[:, j], linewidth=1.1, label=name)\n",
    "        ax.set_title(ex.replace('_', ' ').title(), fontsize=10)\n",
    "        ax.set_xlabel('Frame')\n",
    "    except FileNotFoundError:\n",
    "        ax.set_title(f'{ex}\\n(no data)')\n",
    "\n",
    "axes[0].set_ylabel('Angle (°)')\n",
    "axes[0].legend(fontsize=6, ncol=2)\n",
    "plt.tight_layout()\n",
    "plt.savefig(ROOT / 'evaluation/figures/sample_trajectories.png',\n",
    "            dpi=150, bbox_inches='tight')\n",
    "plt.show()\n"
   ],
   "id": "cell-trajectories"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3  Training History"
   ],
   "id": "cell-md-training"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGS_DIR = ROOT / 'models' / 'logs'\n",
    "\n",
    "fig, axes = plt.subplots(1, len(EXERCISES), figsize=(16, 4), sharey=True)\n",
    "fig.suptitle('Training Curves — Phase 1 (Pre-train on Synthetic Data)', fontsize=13)\n",
    "\n",
    "for ax, ex in zip(axes, EXERCISES):\n",
    "    log_path = LOGS_DIR / f'{ex}_phase1_history.csv'\n",
    "    if not log_path.exists():\n",
    "        ax.set_title(f'{ex}\\n(not trained)')\n",
    "        continue\n",
    "    hist = pd.read_csv(log_path)\n",
    "    ax.plot(hist['epoch'], hist['mae'] * 100,     label='Train MAE', linewidth=1.5)\n",
    "    ax.plot(hist['epoch'], hist['val_mae'] * 100, label='Val MAE',   linewidth=1.5, linestyle='--')\n",
    "    ax.axhline(15, color='red', linestyle=':', linewidth=1, label='Target < 15')\n",
    "    ax.set_title(ex.replace('_', ' ').title(), fontsize=10)\n",
    "    ax.set_xlabel('Epoch')\n",
    "\n",
    "axes[0].set_ylabel('MAE (0–100 scale)')\n",
    "axes[0].legend(fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.savefig(ROOT / 'evaluation/figures/training_curves_phase1.png',\n",
    "            dpi=150, bbox_inches='tight')\n",
    "plt.show()\n"
   ],
   "id": "cell-training-curves"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4  Model Evaluation (MAE · Pearson · ICC)"
   ],
   "id": "cell-md-eval"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.evaluate_model import evaluate_all\n",
    "\n",
    "df_eval = evaluate_all(phase=3, verbose=True)\n",
    "display(df_eval.round(3))\n"
   ],
   "id": "cell-evaluate"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heat map: MAE per exercise × dimension\n",
    "if not df_eval.empty:\n",
    "    pivot = df_eval.pivot(index='exercise', columns='dimension', values='mae')[SCORE_DIMS]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    sns.heatmap(pivot, annot=True, fmt='.1f', cmap='YlOrRd_r',\n",
    "                vmin=0, vmax=20, linewidths=0.5, ax=ax)\n",
    "    ax.set_title('MAE Heatmap — DL Model (Phase 3)', fontsize=13)\n",
    "    ax.set_xlabel('Score Dimension')\n",
    "    ax.set_ylabel('Exercise')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(ROOT / 'evaluation/figures/mae_heatmap.png',\n",
    "                dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n"
   ],
   "id": "cell-mae-heatmap"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5  Three-Way Comparison: Rule-based vs DL vs Hybrid"
   ],
   "id": "cell-md-comparison"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.compare_approaches import compare_all\n",
    "\n",
    "df_cmp = compare_all(phase=3)\n",
    "display(df_cmp.round(3))\n"
   ],
   "id": "cell-compare"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publication-ready comparison bar chart\n",
    "from evaluation.visualize_predictions import hybrid_comparison_bar\n",
    "\n",
    "fig = hybrid_comparison_bar(phase=3, save=True)\n",
    "plt.show()\n"
   ],
   "id": "cell-comparison-bar"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6  Predicted vs Actual Scatter Plots"
   ],
   "id": "cell-md-scatter"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.visualize_predictions import scatter_predicted_vs_actual\n",
    "from models.form_scorer_model import build_form_scorer\n",
    "\n",
    "MODELS_DIR = ROOT / 'models' / 'saved'\n",
    "\n",
    "for ex in EXERCISES:\n",
    "    model_path = MODELS_DIR / f'{ex}_phase3_final.keras'\n",
    "    if not model_path.exists():\n",
    "        print(f'[SKIP] {ex}')\n",
    "        continue\n",
    "    model = tf.keras.models.load_model(str(model_path))\n",
    "    fig   = scatter_predicted_vs_actual(ex, model, save=True)\n",
    "    plt.show()\n"
   ],
   "id": "cell-scatter"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7  Model Architecture Summary"
   ],
   "id": "cell-md-arch"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.form_scorer_model import build_form_scorer, compile_model\n",
    "\n",
    "model = build_form_scorer()\n",
    "model.summary()\n",
    "print(f'\\nTotal trainable parameters: {model.count_params():,}')\n"
   ],
   "id": "cell-architecture"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8  TFLite Export Verification"
   ],
   "id": "cell-md-tflite"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from export.convert_tflite import DEFAULT_OUT\n",
    "\n",
    "tflite_dir = ROOT / 'export' / 'tflite'\n",
    "rows = []\n",
    "for ex in EXERCISES:\n",
    "    path = tflite_dir / f'form_scorer_{ex}_v1.tflite'\n",
    "    if path.exists():\n",
    "        rows.append({'exercise': ex, 'size_KB': round(path.stat().st_size / 1024, 1),\n",
    "                     'status': '✓'})\n",
    "    else:\n",
    "        rows.append({'exercise': ex, 'size_KB': '-', 'status': 'not yet exported'})\n",
    "\n",
    "df_tflite = pd.DataFrame(rows)\n",
    "print('TFLite model files:')\n",
    "display(df_tflite)\n",
    "print(f'\\nTotal: {df_tflite[df_tflite[\"size_KB\"] != \"-\"][\"size_KB\"].astype(float).sum():.1f} KB')\n"
   ],
   "id": "cell-tflite-check"
  }
 ]
}
